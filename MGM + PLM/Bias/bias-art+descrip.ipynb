{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "root = Path('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = {'left': 0, 'center': 1, 'right': 2}\n",
    "classes = list(encoder.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model article deberta-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_article = pd.read_pickle(root / 'probability/deberta-v3-base_train_prob_bias_article.pkl')\n",
    "X_test_article = pd.read_pickle(root / 'probability/deberta-v3-base_test_prob_bias_article.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model description bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_des = pd.read_pickle(root / 'probability/bert-base-uncased_train_prob_bias_description.pkl')\n",
    "X_test_des = pd.read_pickle(root / 'probability/bert-base-uncased_test_prob_bias_description.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataset(df, df_1):\n",
    "    df = df.merge(df_1, on=['website', 'target'])\n",
    "    df['left'] = df['left_x'] + df['left_y']\n",
    "    df['right'] = df['right_x'] + df['right_y']\n",
    "    df['center'] = df['center_x'] + df['center_y']\n",
    "    return df[['target', 'website'] + classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = merge_dataset(X_train_article, X_train_des)\n",
    "X_test = merge_dataset(X_test_article, X_test_des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9298245614035088\n",
      "Macro-F1 Score: 0.9263511101592061\n",
      "Average Recall: 0.9266663237722952\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train[classes], X_train['target'])\n",
    "y_pred = clf.predict(X_test[classes])\n",
    "y_test = X_test['target']\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "avg_recall = recall_score(y_test, y_pred, average='macro')\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Macro-F1 Score:\", macro_f1)\n",
    "print(\"Average Recall:\", avg_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_models_file(X_train, num_model='M1'):\n",
    "    files = []\n",
    "    renames = {'logit_1': 'center', 'logit_2':'right', 'logit_3':'left'}\n",
    "    for file in os.listdir(root / 'step_3' / num_model):\n",
    "        if file.endswith('csv'):\n",
    "            df = pd.read_csv(root / 'step_3' / num_model / file)\n",
    "            df.rename(columns=renames, inplace=True)\n",
    "            df['target'] = None\n",
    "            for index, row in df.iterrows():\n",
    "                df.iloc[index, -1] = row.index[row[1:].argmax() + 1]\n",
    "            df = pd.concat([X_train, df])\n",
    "            files.append(df)\n",
    "    \n",
    "    return files\n",
    "\n",
    "def evaluate(files: list[pd.DataFrame], X_test: pd.DataFrame, params_log_reg: dict):\n",
    "    accuracy_list = []\n",
    "    macro_f1_list = []\n",
    "    avg_recall_list = []\n",
    "    for index, X_train in enumerate(files):\n",
    "        clf = LogisticRegression(**params_log_reg)\n",
    "        clf.fit(X_train[classes], X_train['target'])\n",
    "        y_pred = clf.predict(X_test[classes])\n",
    "        y_test = X_test['target']\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        avg_recall = recall_score(y_test, y_pred, average='macro')\n",
    "        accuracy_list.append(accuracy), macro_f1_list.append(macro_f1), avg_recall_list.append(avg_recall)\n",
    "        print(f'Index: {index}')\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(\"Macro-F1 Score:\", macro_f1)\n",
    "        print(\"Average Recall:\", avg_recall)\n",
    "    accuracy_std = np.std(accuracy_list)\n",
    "    macro_f1_std = np.std(macro_f1_list)\n",
    "    avg_recall_std = np.std(avg_recall_list)\n",
    "\n",
    "    print(f\"Standard Deviation of Accuracy: {accuracy_std}\")\n",
    "    print(f\"Standard Deviation of Macro-F1 Score: {macro_f1_std}\")\n",
    "    print(f\"Standard Deviation of Average Recall: {avg_recall_std}\")\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0\n",
      "Accuracy: 0.9298245614035088\n",
      "Macro-F1 Score: 0.9260152632883744\n",
      "Average Recall: 0.9246965861055391\n",
      "Index: 1\n",
      "Accuracy: 0.935672514619883\n",
      "Macro-F1 Score: 0.9319683711603318\n",
      "Average Recall: 0.9332022714847135\n",
      "Index: 2\n",
      "Accuracy: 0.935672514619883\n",
      "Macro-F1 Score: 0.9319683711603318\n",
      "Average Recall: 0.9332022714847135\n",
      "Index: 3\n",
      "Accuracy: 0.935672514619883\n",
      "Macro-F1 Score: 0.9319683711603318\n",
      "Average Recall: 0.9332022714847135\n",
      "Index: 4\n",
      "Accuracy: 0.935672514619883\n",
      "Macro-F1 Score: 0.9319683711603318\n",
      "Average Recall: 0.9332022714847135\n",
      "Standard Deviation of Accuracy: 0.002339181286549685\n",
      "Standard Deviation of Macro-F1 Score: 0.0023812431487829854\n",
      "Standard Deviation of Average Recall: 0.00340227415166976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files = read_models_file(X_train, num_model='M1')\n",
    "params = {\n",
    "    'C': 2, 'penalty': 'l1',\n",
    "    'solver': 'liblinear',\n",
    "}\n",
    "evaluate(files, X_test, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.9345029240 ± 0.0023\n",
    "Macro-F1 Score: 0.9307777496 ± 0.0024\n",
    "Average Recall: 0.9315011344 ± 0.0034"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0\n",
      "Accuracy: 0.935672514619883\n",
      "Macro-F1 Score: 0.9309296911244568\n",
      "Average Recall: 0.9332022714847135\n",
      "Index: 1\n",
      "Accuracy: 0.935672514619883\n",
      "Macro-F1 Score: 0.9319683711603318\n",
      "Average Recall: 0.9332022714847135\n",
      "Index: 2\n",
      "Accuracy: 0.9298245614035088\n",
      "Macro-F1 Score: 0.9252816180235536\n",
      "Average Recall: 0.9266663237722952\n",
      "Index: 3\n",
      "Accuracy: 0.935672514619883\n",
      "Macro-F1 Score: 0.9319683711603318\n",
      "Average Recall: 0.9332022714847135\n",
      "Index: 4\n",
      "Accuracy: 0.935672514619883\n",
      "Macro-F1 Score: 0.9319683711603318\n",
      "Average Recall: 0.9332022714847135\n",
      "Standard Deviation of Accuracy: 0.002339181286549685\n",
      "Standard Deviation of Macro-F1 Score: 0.002602116837811581\n",
      "Standard Deviation of Average Recall: 0.0026143790849673105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files = read_models_file(X_train, num_model='M2')\n",
    "params = {\n",
    "    'C': 2, 'penalty': 'l1',\n",
    "    'solver': 'liblinear',\n",
    "}\n",
    "evaluate(files, X_test, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.9345029240 ± 0.0023\n",
    "Macro-F1 Score: 0.9304232845 ± 0.0026\n",
    "Average Recall: 0.9318950819 ± 0.0026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0\n",
      "Accuracy: 0.935672514619883\n",
      "Macro-F1 Score: 0.9319683711603318\n",
      "Average Recall: 0.9332022714847135\n",
      "Index: 1\n",
      "Accuracy: 0.9298245614035088\n",
      "Macro-F1 Score: 0.9260152632883744\n",
      "Average Recall: 0.9246965861055391\n",
      "Index: 2\n",
      "Accuracy: 0.9239766081871345\n",
      "Macro-F1 Score: 0.9195986994096005\n",
      "Average Recall: 0.9201303760598769\n",
      "Index: 3\n",
      "Accuracy: 0.935672514619883\n",
      "Macro-F1 Score: 0.9319683711603318\n",
      "Average Recall: 0.9332022714847135\n",
      "Index: 4\n",
      "Accuracy: 0.9298245614035088\n",
      "Macro-F1 Score: 0.9260152632883744\n",
      "Average Recall: 0.9246965861055391\n",
      "Standard Deviation of Accuracy: 0.004376207469911036\n",
      "Standard Deviation of Macro-F1 Score: 0.004604867493438223\n",
      "Standard Deviation of Average Recall: 0.005187817203548877\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files = read_models_file(X_train, num_model='M3')\n",
    "params = {\n",
    "    'C': 2, 'penalty': 'l1',\n",
    "    'solver': 'liblinear',\n",
    "}\n",
    "evaluate(files, X_test, params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
